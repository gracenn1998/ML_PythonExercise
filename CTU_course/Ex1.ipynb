{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaning-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blessed-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_of_2_examples(u, v, q=1):\n",
    "    d_power = np.power(np.abs(np.subtract(u, v)), q)\n",
    "    distance = np.power(np.sum(d_power, axis=0), 1/q)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sixth-honey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_of_1set_1ex(set1, ex, q=1):\n",
    "    m = set1.shape[0]\n",
    "    d_set = np.zeros((m, 1))\n",
    "    for i in range(m):\n",
    "        d_set[i] = distance_of_2_examples(set1[i], ex, q)\n",
    "    d_set.reshape(m, 1)\n",
    "    return d_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "imposed-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_list(arr):\n",
    "    class_list = np.unique(arr)\n",
    "    return class_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diagnostic-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "def load_dataset(train_set_file_name, test_set_file_name):\n",
    "        \n",
    "    train_set = np.loadtxt(train_set_file_name, delimiter=',')\n",
    "    test_set = np.loadtxt( test_set_file_name, delimiter=',')\n",
    "    \n",
    "    dims = np.size(train_set, 1) - 1 #number of col - 1 = dim, last col is class label\n",
    "   \n",
    "    train_x = train_set[:, :dims]\n",
    "    train_y = train_set[:, -1:]\n",
    "    test_x = test_set[:, :dims]\n",
    "    test_y = test_set[:, -1:]\n",
    "    \n",
    "    \n",
    "    class_list = get_class_list(train_y)\n",
    "    \n",
    "    #if test_y == -1 => class = -1 vs 1 => let it become 0 - 1 for generalization\n",
    "    #replace -1 -> 0\n",
    "#     train_y = np.where(train_y==-1.0, 0, train_y)\n",
    "#     test_y = np.where(test_y==-1.0, 0, test_y)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "supreme-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_idx(arr, k):\n",
    "    return np.argpartition(arr, -k)[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict, test_y):\n",
    "    isCorrect = []\n",
    "    isCorrect = (test_y == predict)\n",
    "    correct_cnt = np.count_nonzero(isCorrect==True)\n",
    "    total_cnt = isCorrect.shape[0]\n",
    "    \n",
    "    acc = correct_cnt/total_cnt\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bright-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predict, test_y, class_list):\n",
    "    class_num = class_list.size\n",
    "    matrix = np.zeros((class_num, class_num), dtype=int) #matrix[i][j] = predicted j for actual lable i \n",
    "    \n",
    "    label_idx = class_list\n",
    "    if(np.amax(class_list)==class_num):\n",
    "        label_idx = class_list - 1  # for dataset labeled from 1 to n -> let label for indexing be 0 -> n-1\n",
    "    \n",
    "    \n",
    "    for i in label_idx:\n",
    "        if (i==-1): #consider label -1 as 0 for indexing\n",
    "            actual_class_label_idx = 0 #label from 1->n or 0->n wont be affected\n",
    "        else:\n",
    "            actual_class_label_idx = int(i)\n",
    "        \n",
    "        idx = np.where(test_y==i) #list of idx where actual label = i\n",
    "        \n",
    "        for j in label_idx:\n",
    "            if(j==-1): #consider label -1 as 0 for indexing\n",
    "                predicted_class_label_idx = 0 #label from 1->n or 0->n wont be affected\n",
    "            else:\n",
    "                predicted_class_label_idx = int(j)\n",
    "            \n",
    "            matrix[actual_class_label_idx][predicted_class_label_idx] = int(np.count_nonzero(predict[idx]==j))\n",
    "    \n",
    "    return matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "found-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    min_val = np.amin(arr)\n",
    "    max_val = np.amax(arr)\n",
    "    \n",
    "    arr = (arr-min_val)/(max_val-min_val)\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sudden-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(train_set_file_name, test_set_file_name, k):\n",
    "    #get dataset\n",
    "    train_x, train_y, test_x, test_y, class_list = load_dataset(train_set_file_name, test_set_file_name)\n",
    "    class_num = class_list.size\n",
    "    \n",
    "    #get dataset size\n",
    "    m_train = train_x.shape[0]\n",
    "    m_test = test_x.shape[0]\n",
    "    \n",
    "    #normalizaion\n",
    "    train_x_normalized = normalize(train_x)\n",
    "    test_x_normalized = normalize(test_x)\n",
    "    \n",
    "    \n",
    "    d_set = np.zeros((m_test, m_train)) #distance of  ith example in test compare to all examples in train\n",
    "    d_set_top_k_idx = np.zeros((m_test, k), dtype=int) #top k index of k nearest examples\n",
    "    class_of_top_knn = np.zeros((m_test, k), dtype=int) \n",
    "    class_occurence = np.zeros((m_test, class_num+1), dtype=int) #o[i][j]: occurence of class j in ith example\n",
    "    predict = np.zeros((m_test, 1), dtype=int)\n",
    "    \n",
    "    #get top k classes\n",
    "    for i in range(m_test):\n",
    "        #get distance of ith example in test compare to all examples in train\n",
    "        #each row is a list of distance compare to each example in train\n",
    "        d_set[i] = distance_of_1set_1ex(train_x_normalized, test_x_normalized[i]).reshape(1,-1)\n",
    "        \n",
    "        #sort to get k min distance\n",
    "        #idx = np.argpartition(x, -k)[-k:]  # Indices of top k max, idx not sorted\n",
    "        d_set_top_k_idx[i] = np.argpartition(d_set[i], k)[:k]\n",
    "        \n",
    "        #get ith example's class list of top knn\n",
    "        class_of_top_knn[i] = train_y[d_set_top_k_idx[i]].reshape(1,-1)\n",
    "        \n",
    "        \n",
    "        #get class with highest occurence of example ith\n",
    "        for j in class_list:\n",
    "            \n",
    "            if(j==-1): #if class = -1 -> consider as 0\n",
    "                class_label = 0\n",
    "            else:\n",
    "                class_label = int(j)\n",
    "            class_occurence[i][class_label] = np.sum(class_of_top_knn[i]==j)\n",
    "            \n",
    "        #predicted class is the class with hight occurence\n",
    "        most_occurence_class = np.squeeze(np.where(class_occurence[i]==np.max(class_occurence[i])))\n",
    "        if most_occurence_class.size==1:\n",
    "            predict[i] = most_occurence_class\n",
    "        else:\n",
    "            #get a random class if many classes have same occurence\n",
    "            predict[i] = np.random.choice(most_occurence_class)\n",
    "    \n",
    "    \n",
    "    acc = accuracy(predict, test_y)\n",
    "    confusion_arr = confusion_matrix(predict, test_y, class_list)\n",
    "    print(\"Accuracy: \" + str(acc*100) + \"%\")\n",
    "    \n",
    "    print(\"Confusion matrix (col header is actual val, row header is predicted val): \")\n",
    "    label_list = [str(int(i)) for i in class_list]\n",
    "    df = pd.DataFrame.from_records(data=confusion_arr, columns=label_list)\n",
    "    df.index = label_list\n",
    "    print(df)\n",
    "    \n",
    "    return predict, acc, confusion_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.0%\n",
      "Confusion matrix (col header is actual val, row header is predicted val): \n",
      "    0   1   2\n",
      "0  17   0   0\n",
      "1   0  15   0\n",
      "2   0   4  14\n",
      "==============\n",
      "Accuracy: 74.375%\n",
      "Confusion matrix (col header is actual val, row header is predicted val): \n",
      "    1   2  3   4  5  6  7  8  9  10  11  12  13  14  15\n",
      "1   0   0  0   0  0  0  0  0  0   0   0   0   0   0   0\n",
      "2   0  29  0   0  0  0  0  0  0   0   0   0   0   0   0\n",
      "3   0   0  3   0  0  0  0  0  0   0   0   1   0   0   0\n",
      "4   0   1  0  10  0  0  0  0  0   0   0   1   0   0   0\n",
      "5   0   2  0   0  4  0  0  0  0   0   0   0   1   0   0\n",
      "6   0   0  0   0  0  8  0  1  0   0   0   0   0   0   0\n",
      "7   0  11  0   0  0  0  2  0  0   0   0   1   0   0   0\n",
      "8   0   1  0   0  0  0  0  9  0   0   0   0   0   0   0\n",
      "9   0   1  0   0  0  0  0  0  9   0   0   1   0   0   0\n",
      "10  0   0  0   0  0  0  0  0  0   4   0   3   0   0   0\n",
      "11  0   0  0   0  0  0  0  0  0   0   2   2   0   0   0\n",
      "12  0   0  0   0  0  0  0  0  0   0   0  10   0   0   0\n",
      "13  0   1  0   0  0  0  0  0  0   0   0   0   8   0   0\n",
      "14  0   2  0   0  0  0  0  1  0   0   0   1   1   5   0\n",
      "15  0   2  0   0  0  0  0  2  0   0   0   3   0   0   3\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "predict = {}\n",
    "predict[\"iris\"] = kNN('dataset/iris.trn', 'dataset/iris.tst', k)\n",
    "print(\"==============\")\n",
    "predict[\"fp\"] = kNN('dataset/fp.trn', 'dataset/fp.tst', k)\n",
    "print(\"==============\")\n",
    "predict[\"letter\"]= kNN('dataset/let.trn', 'dataset/let.tst', k)\n",
    "print(\"==============\")\n",
    "predict[\"optics\"]= kNN('dataset/opt.trn', 'dataset/opt.tst', k)\n",
    "print(\"==============\")\n",
    "predict[\"leukemia\"]= kNN('dataset/ALLAML.trn', 'dataset/ALLAML.tst', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-finland",
   "metadata": {},
   "source": [
    "==============================\n",
    "below is testing cell, meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, train_y, test_x, test_y, classes = load_dataset('dataset/iris.trn', 'dataset/iris.tst')\n",
    "predict = kNN('dataset/iris.trn', 'dataset/iris.tst', k)\n",
    "idx = np.where(test_y==2)\n",
    "print(idx)\n",
    "print(predict[idx])\n",
    "np.count_nonzero(predict[idx]==1)\n",
    "\n",
    "np.arange(0, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# col_title = [0, 1 , 2, 3]\n",
    "row_title = label_list = [str(i) for i in range(3+1)]\n",
    "matrix = confusion_matrix(predict, test_y, 3)\n",
    "print(matrix)\n",
    "df = pd.DataFrame.from_records(data=matrix, index =row_title)\n",
    "print(df)\n",
    "# print(col_title)\n",
    "# print(confusion_matrix(predict, test_y, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5, 5])\n",
    "print(arr)\n",
    "idx = np.argpartition(arr, 3)[:3]\n",
    "print(arr[idx])\n",
    "o = np.zeros((1, 6))\n",
    "for i in range(6):\n",
    "    o[0][i] = np.sum(arr==i)\n",
    "print(o[0])\n",
    "predict = np.max(o[0])\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex1.1 data\n",
    "train_x = np.array([[0.376000, 0.488000],\n",
    "                  [0.312000, 0.544000],\n",
    "                  [0.298000, 0.624000],\n",
    "                  [0.394000, 0.600000],\n",
    "                  [0.506000, 0.512000],\n",
    "                  [0.488000, 0.334000],\n",
    "                  [0.478000, 0.398000],\n",
    "                  [0.606000, 0.366000],\n",
    "                  [0.428000, 0.294000],\n",
    "                  [0.542000, 0.252000]]\n",
    "                  )\n",
    "train_y = np.array([[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]])\n",
    "\n",
    "test_x = np.array([[0.550000, 0.364000],\n",
    "                  [0.558000, 0.470000],\n",
    "                  [0.456000, 0.450000],\n",
    "                  [0.450000, 0.570000]]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-compiler",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
