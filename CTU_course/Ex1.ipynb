{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alive-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excited-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_of_2_examples(u, v, q=1):\n",
    "    d_power = np.power(np.abs(np.subtract(u, v)), q)\n",
    "    distance = np.power(np.sum(d_power, axis=0), 1/q)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forced-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_of_1set_1ex(set1, ex, q=1):\n",
    "    m = set1.shape[0]\n",
    "    d_set = np.zeros((m, 1))\n",
    "    for i in range(m):\n",
    "        d_set[i] = distance_of_2_examples(set1[i], ex, q)\n",
    "    d_set.reshape(m, 1)\n",
    "    return d_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "micro-outline",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-302678ffa320>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-302678ffa320>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def get_class_list(arr):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mental-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "def load_dataset(train_set_file_name, test_set_file_name):\n",
    "        \n",
    "    train_set = np.loadtxt(train_set_file_name, delimiter=',')\n",
    "    test_set = np.loadtxt( test_set_file_name, delimiter=',')\n",
    "    \n",
    "    dims = np.size(train_set, 1) - 1 #number of col - 1 = dim, last col is class label\n",
    "    max_class = int(np.amax(train_set[:, dims])) #max value of label\n",
    "   \n",
    "    train_x = train_set[:, :dims]\n",
    "    train_y = train_set[:, -1:]\n",
    "    test_x = test_set[:, :dims]\n",
    "    test_y = test_set[:, -1:]\n",
    "    \n",
    "    #if test_y == -1 => class = -1 vs 1 => let it become 0 - 1 for generalization\n",
    "    #replace -1 -> 0\n",
    "    train_y = np.where(train_y==-1.0, 0, train_y)\n",
    "    test_y = np.where(test_y==-1.0, 0, test_y)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "uniform-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_idx(arr, k):\n",
    "    return np.argpartition(arr, -k)[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "protective-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predict, test_y):\n",
    "    isCorrect = []\n",
    "    isCorrect = (test_y == predict)\n",
    "    correct_cnt = np.count_nonzero(isCorrect==True)\n",
    "    total_cnt = isCorrect.shape[0]\n",
    "    \n",
    "    acc = correct_cnt/total_cnt\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opposed-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(predict, test_y, classes):\n",
    "    matrix = np.zeros((classes+1, classes+1)) #matrix[i][j] = predicted j for actual lable i\n",
    "    for i in range(classes+1):\n",
    "        #list of idx where actual label = i\n",
    "        idx = np.where(test_y==i)\n",
    "        for j in range(classes+1):\n",
    "            matrix[i][j] = int(np.count_nonzero(predict[idx]==j))\n",
    "    return matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "native-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):\n",
    "    min_val = np.amin(arr)\n",
    "    max_val = np.amax(arr)\n",
    "    \n",
    "    arr = (arr-min_val)/(max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "extensive-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN(train_set_file_name, test_set_file_name, k):\n",
    "    #get dataset\n",
    "    train_x, train_y, test_x, test_y, max_class = load_dataset(train_set_file_name, test_set_file_name)\n",
    "    \n",
    "    #get dataset size\n",
    "    m_train = train_x.shape[0]\n",
    "    m_test = test_x.shape[0]\n",
    "    \n",
    "    #normalizaion\n",
    "    train_x_normalized = normalize(train_x)\n",
    "    test_x_normalized = normalize(test_x)\n",
    "    \n",
    "    \n",
    "    d_set = np.zeros((m_test, m_train)) #distance of  ith example in test compare to all examples in train\n",
    "    d_set_top_k_idx = np.zeros((m_test, k), dtype=int) #top k index of k nearest examples\n",
    "    class_of_top_knn = np.zeros((m_test, k), dtype=int) \n",
    "    class_occurence = np.zeros((m_test, max_class+1), dtype=int) #o[i][j]: occurence of class j in ith example\n",
    "    predict = np.zeros((m_test, 1), dtype=int)\n",
    "    \n",
    "    #get top k classes\n",
    "    for i in range(m_test):\n",
    "        #get distance of ith example in test compare to all examples in train\n",
    "        #each row is a list of distance compare to each example in train\n",
    "        d_set[i] = distance_of_1set_1ex(train_x_normalized, test_x_normalized[i]).reshape(1,-1)\n",
    "        \n",
    "        #sort to get k min distance\n",
    "        #idx = np.argpartition(x, -k)[-k:]  # Indices of top k max, idx not sorted\n",
    "        d_set_top_k_idx[i] = np.argpartition(d_set[i], k)[:k]\n",
    "        \n",
    "        #get ith example's class list of top knn\n",
    "        class_of_top_knn[i] = train_y[d_set_top_k_idx[i]].reshape(1,-1)\n",
    "        \n",
    "        #get class with highest occurence = predict result\n",
    "        for j in range(max_class+1):\n",
    "            class_occurence[i][j] = np.sum(class_of_top_knn[i]==j)\n",
    "        \n",
    "        #predicted class is the class with hight occurence\n",
    "        most_occurence_class = np.squeeze(np.where(class_occurence[i]==np.max(class_occurence[i])))\n",
    "        if most_occurence_class.size==1:\n",
    "            predict[i] = most_occurence_class\n",
    "        else:\n",
    "            #get a random class if many classes have same occurence\n",
    "            predict[i] = np.random.choice(most_occurence_class)\n",
    "#         print(i)\n",
    "#         print(np.where(class_occurence[i]==np.max(class_occurence[i])))\n",
    "#         print(most_occurence_class)\n",
    "#         print( predict[i])\n",
    "    acc = accuracy(predict, test_y)\n",
    "    confusion_arr = confusion_matrix(predict, test_y, max_class)\n",
    "    print(\"Accuracy: \" + str(acc*100) + \"%\")\n",
    "    \n",
    "    print(\"Confusion matrix (col header is actual val, row header is predicted val): \")\n",
    "    label_list = [str(i) for i in range(max_class+1)]\n",
    "    df = pd.DataFrame.from_records(data=confusion_arr, index=label_list)\n",
    "    print(df)\n",
    "    \n",
    "    return predict, acc, confusion_arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "liberal-peeing",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-26bb1e3c7736>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"iris\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset/iris.trn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dataset/iris.tst'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"==============\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# predict[\"fp\"] = kNN('dataset/fp.trn', 'dataset/fp.tst', k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-6c92527885db>\u001b[0m in \u001b[0;36mkNN\u001b[1;34m(train_set_file_name, test_set_file_name, k)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#get distance of ith example in test compare to all examples in train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#each row is a list of distance compare to each example in train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0md_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_of_1set_1ex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_normalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x_normalized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#sort to get k min distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "predict = {}\n",
    "predict[\"iris\"] = kNN('dataset/iris.trn', 'dataset/iris.tst', k)\n",
    "print(\"==============\")\n",
    "# predict[\"fp\"] = kNN('dataset/fp.trn', 'dataset/fp.tst', k)\n",
    "# print(\"==============\")\n",
    "# predict[\"letter\"]= kNN('dataset/let.trn', 'dataset/let.tst', k)\n",
    "# print(\"==============\")\n",
    "# predict[\"optics\"]= kNN('dataset/opt.trn', 'dataset/opt.tst', k)\n",
    "# print(\"==============\")\n",
    "predict[\"leukemia\"]= kNN('dataset/ALLAML.trn', 'dataset/ALLAML.tst', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-employer",
   "metadata": {},
   "source": [
    "==============================\n",
    "below is testing cell, meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, train_y, test_x, test_y, classes = load_dataset('dataset/iris.trn', 'dataset/iris.tst')\n",
    "predict = kNN('dataset/iris.trn', 'dataset/iris.tst', k)\n",
    "idx = np.where(test_y==2)\n",
    "print(idx)\n",
    "print(predict[idx])\n",
    "np.count_nonzero(predict[idx]==1)\n",
    "\n",
    "np.arange(0, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# col_title = [0, 1 , 2, 3]\n",
    "row_title = label_list = [str(i) for i in range(3+1)]\n",
    "matrix = confusion_matrix(predict, test_y, 3)\n",
    "print(matrix)\n",
    "df = pd.DataFrame.from_records(data=matrix, index =row_title)\n",
    "print(df)\n",
    "# print(col_title)\n",
    "# print(confusion_matrix(predict, test_y, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, 3, 4, 5, 5])\n",
    "print(arr)\n",
    "idx = np.argpartition(arr, 3)[:3]\n",
    "print(arr[idx])\n",
    "o = np.zeros((1, 6))\n",
    "for i in range(6):\n",
    "    o[0][i] = np.sum(arr==i)\n",
    "print(o[0])\n",
    "predict = np.max(o[0])\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex1.1 data\n",
    "train_x = np.array([[0.376000, 0.488000],\n",
    "                  [0.312000, 0.544000],\n",
    "                  [0.298000, 0.624000],\n",
    "                  [0.394000, 0.600000],\n",
    "                  [0.506000, 0.512000],\n",
    "                  [0.488000, 0.334000],\n",
    "                  [0.478000, 0.398000],\n",
    "                  [0.606000, 0.366000],\n",
    "                  [0.428000, 0.294000],\n",
    "                  [0.542000, 0.252000]]\n",
    "                  )\n",
    "train_y = np.array([[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]])\n",
    "\n",
    "test_x = np.array([[0.550000, 0.364000],\n",
    "                  [0.558000, 0.470000],\n",
    "                  [0.456000, 0.450000],\n",
    "                  [0.450000, 0.570000]]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-affiliation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
